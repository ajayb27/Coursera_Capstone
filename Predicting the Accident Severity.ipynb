{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Capstone project - Predicting the Car Accident Severity"}, {"metadata": {}, "cell_type": "markdown", "source": "### Applied Data Science Capstone by IBM/Coursera"}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of contents\n- [Introduction: Business Problem](#business_problem)\n- [Data](#data)\n- [Methodology](#methodology)\n- [Analysis](#analysis)\n- [Results and Discussion](#results)\n- [Conclusion](#conclusion)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id='business_problem'></a>\n## Business Problem \n\nSeattle, a city on Puget Sound in the Pacific Northwest, is surrounded by water, mountains and evergreen forests, and contains thousands of acres of parkland. Washington State\u2019s largest city, it\u2019s home to a large tech industry, with Microsoft and Amazon headquartered in its metropolitan area. The traffic is also huge as you guess and accidents are very common.\n\nIt is an challenge to government to control the accidents. The data of previous data were taken and now the task is to make better use of the available data. Many of the accidents cases in the city are because of the negligance of the people driving the vehicles. But also there are cases of some uncotrollable factors like light, weather, roads, etc. So the accidents because of these uncontrollable factors can be controlled by using the previous data and making an efficient solution out from it. For example, an alert can be sent to the drivers predicting the chances of accident to take place based on the factors previously mentioned.\n\nThe target audience of this project are Government of Seattle, local police and rescue teams, also for car financing corporations. They can gain a lot of profit from implementing this thing.  \n\nWe will use our Data Science technology to make out an absolute working solution for it now."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"data\"></a>\n## Data\nThe data collected here was huge and was collected by the Seattle Police Department and Accident Traffic Records Department from 2004 to present. \nThe data consists of 37 independent variables and 194,673 rows.\n\nDepending on the definition of our problem, factors that will influence our decission are:\n- Road condition\n- Weather condition\n- Light condition"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Importing all required packages"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn import preprocessing\nfrom matplotlib.ticker import NullFormatter\n%matplotlib inline", "execution_count": 150, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Extracting Data"}, {"metadata": {}, "cell_type": "code", "source": "pre_df = pd.read_csv(\"https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv\")\npre_df.head()\npre_df.columns\"", "execution_count": 151, "outputs": [{"output_type": "error", "ename": "SyntaxError", "evalue": "EOL while scanning string literal (<ipython-input-151-e2a3b71c4a42>, line 3)", "traceback": ["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-151-e2a3b71c4a42>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pre_df.columns\"\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Removing unnecessary data"}, {"metadata": {}, "cell_type": "code", "source": "pre_df = pre_df[['SEVERITYCODE', 'ROADCOND', 'WEATHER', 'LIGHTCOND']]\npre_df.head()\n\npre_df = pre_df.dropna()\npre_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Data processing\n\nAnyhow, we have to prepare the data is not a form to analyse it. In order to prepare the data, first, we need to drop the non-relevant columns. Last but not least remove the values containing null.\n\nAfter studying the data, as per our requirements mentioned above  I have decided to pick up three independent variables - light condition, weather condition and road condition and severity code as the target variable. \n\nOur target variable is \"SEVERITYCODE\", contains numbers that correspond to different levels of severity caused by an accident from 0 to 4.\n\nSeverity codes are as follows:\n\n0. Little to no Probability (Clear Conditions)\n1. Very Low Probability \u2014 Chance or Property Damage\n2. Low Probability \u2014 Chance of Injury\n3. Mild Probability \u2014 Chance of Serious Injury\n4. High Probability \u2014 Chance of Fatality\n\nLet us look at the count of various sevirity codes in the data set"}, {"metadata": {}, "cell_type": "code", "source": "pre_df[\"SEVERITYCODE\"].value_counts().plot(kind='bar')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can see that the number of rows in class 1 is much bigger than the number of rows in class 2. We can simply downsample the class 1 to make the data set unbiased."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.utils import resample\npre_df_major = pre_df[pre_df.SEVERITYCODE==1]\npre_df_minor = pre_df[pre_df.SEVERITYCODE==2]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "maj_sample = resample(pre_df_major,replace=False,n_samples=58188,random_state=13)\n\nbal_df = pd.concat([maj_sample,pre_df_minor])\n\nbal_df[\"SEVERITYCODE\"].value_counts().plot(kind='bar')\n\nbal_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we are ready for the next step."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id='methodology'></a>\n## Methodology\n\nAs I mentioned earlier <b>WEATHER</b>, <b>ROADCOND</b>, <b>LIGHTCOND</b> are the factors predicting the results more accurately.\nOur data was just prepared enough to get the target variable <b>SEVERITYCODE</b>."}, {"metadata": {}, "cell_type": "code", "source": "pre_df[\"WEATHER\"].value_counts().plot(kind='bar')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pre_df[\"ROADCOND\"].value_counts().plot(kind='bar')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pre_df[\"LIGHTCOND\"].value_counts().plot(kind='bar')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Feature = pre_df[['SEVERITYCODE']]\ndf = pd.concat([Feature, pd.get_dummies(pre_df[['WEATHER','ROADCOND','LIGHTCOND']])],axis=1)\ndf.drop(['SEVERITYCODE'],axis=1,inplace=True)\nFeature.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df.values\nX[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y=Feature['SEVERITYCODE'].values\ny[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Feature shape:\", df.shape)\nprint(\"X shape:\",X.shape)\nprint (\"y shape:\", y.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The data look pretty good for the modelling."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id='analysis'></a>\n## Analysis\n\n### Splitting data set"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nprint('train set:', X_train.shape, y_train.shape)\nprint('test set:', X_test.shape, y_test.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Decision Tree Model"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Modelling phase\nAccident_Severity_Model=DecisionTreeClassifier(criterion='entropy', max_depth=5)\nAccident_Severity_Model.fit(X_train, y_train)", "execution_count": null, "outputs": [{"output_type": "execute_result", "execution_count": 153, "data": {"text/plain": "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#Predicting phase\nyhat_d=Accident_Severity_Model.predict(X_test)\nprint(yhat_d [0:5])\nprint(y_test [0:5])", "execution_count": null, "outputs": [{"output_type": "stream", "text": "[1 1 1 1 1]\n[1 1 1 1 2]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#Accuracy of the model using sklearn\nfrom sklearn import metrics\nprint(\"Decision Tress Accuracy:\", metrics.accuracy_score(y_test, yhat_d))", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Decision Tress Accuracy: 0.6961550649625013\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### KNN MODEL"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.neighbors import KNeighborsClassifier\nk = 25", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)\nneigh", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "yhat_k = neigh.predict(X_test)\nyhat_k[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Logistic Regression"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=6,solver='liblinear').fit(X_train,y_train)\nLR", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "yhat_lr = LR.predict(X_test)\nyhat_lr[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "yhat_proba = LR.predict_proba(X_test)\nyhat_proba", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id='results'></a>\n## Results and Discussions"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Desicion Tree"}, {"metadata": {}, "cell_type": "code", "source": "js_d = jaccard_similarity_score(y_test,yhat_d)\nfs_d = f1_score(y_test,yhat_d,average='macro')\nprint(js_d,fs_d)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### K-Nearest Neighbors"}, {"metadata": {}, "cell_type": "code", "source": "js_knn = jaccard_similarity_score(y_test,yhat_k)\nfs_knn = jaccard_similarity_score(y_test,yhat_k)\nprint(js_knn,fs_knn)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Logistic Regression"}, {"metadata": {}, "cell_type": "code", "source": "js_lr = jaccard_similarity_score(y_test,yhat_lr)\nfs_lr = jaccard_similarity_score(y_test,yhat_lr)\nprint(js_lr,fs_lr)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "list_js = [js_d,js_knn,js_lr]\nlist_fs = [fs_d,fs_knn,fs_lr]\nlist_index = [\"Decision Tree\",\"KNN\",\"Logistic Regression\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data = {'Jaccard Score':[js_d,js_knn,js_lr],'F1 Score':[fs_d,fs_knn,fs_lr]}\nres = pd.DataFrame(data, index=list_index)\nres.reset_index()\nres\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Discussion\nIn the beginning of this notebook, we had categorical data that was of type 'object'. This is not a data type that we could have fed through an algoritim, so label encoding was used to created new classes that were of type int8; a numerical data type.\n\nAfter solving that issue we were presented with another - imbalanced data. As mentioned earlier, class 1 was nearly three times larger than class 2. The solution to this was downsampling the majority class with sklearn's resample tool. We downsampled to match the minority class exactly with 58188 values each.\n\nOnce we analyzed and cleaned the data, it was then fed through three ML models; K-Nearest Neighbor, Decision Tree and Logistic Regression. Although the first two are ideal for this project, logistic regression made most sense because of its binary nature."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id='conclusion'></a>\n## Conclusion\n\nWe got a confirmation of the impact the factors <b>WEATHER</b>, <b>ROADCOND</b>, <b>LIGHTCOND</b> on the accidents and built a model predicting the <b>SEVERITYCODE</b> indicating the severity of the accident. Finally, this model can be used to determine the probability of the accident taking place and upto what extent would be the damage."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}